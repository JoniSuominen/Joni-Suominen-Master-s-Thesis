\chapter{Conclusions}
\label{ch:conclusion}

In this thesis we have presented a novel spline-based method for colour correction. The algorithm was built to extend the previous ideas in the literature, namely polynomial colour correction and its variants, as splines are also polynomials but are defined piecewise. The motive behind this research topic was, in general, to investigate the nature of colour transformations and to see if we can do better than the current algorithms that are simple in their design. They have performed well in practice, as seen by the continued use of a simple linear model for correction in the industry. However, expected advancements in display technology will likely lead to larger colour gamuts, necessitating more complex transformations. For example, in virtual and augmented reality (VR and AR) applications, errors in faithfully reproducing colours would likely result in loss of immersion for the user. 

A key contribution to this idea arises from the work of \citeauthor{finlayson2015color} in their publication on Root-Polynomial Colour Correction. In their paper, they stated, "From the low dimensional assumption of the reflectance spectra and a relatively good performance of the linear transform, we know that the mapping is approximately linear. Thus, we can expect that any non-linearities will be smooth across the sensor domain...". \cite[10]{finlayson2015color} 

Building on the insights, we took an approach of Penalized B-splines (P-splines), which enable the building of smooth yet non-linear models due to the ability to penalise the smoothness. To evaluate the performance of our model, we built a test bench in Python for colour correction algorithms since none exist, aside from the Colour Correction Toolbox \cite{fang2017colour} written in MATLAB, and it has not been maintained as of 2017. With the ever-growing adaptation of Python in the scientific community \cite{ozgur2017matlab}, we also wish to contribute and open-source our software for anyone.

The tests were conducted by generating synthetic response data for both the cameras and target human responses. This allowed us to experiment with various objects found in real life, such as skin tones, vegetation and human-made objects, collected from public datasets of hyperspectral images \cite{foster:2002}, \cite{Foster2022}, \cite{CAVE_0293}, \cite{wueller2009situ}. This allowed us to test the algorithms' real-life performance and the effect of training set size.

In this thesis, we sought to answer three research questions. First, we examined how our proposed algorithm fares against competitors, notably the PCC and RPCC models of different orders. We used the perceptual CIEDE2000 metric to assess the colour accuracy and observed that the proposed algorithms performed comparably to already established models despite not undergoing specialised optimisation. Furthermore, we did not notice a significant improvement in performance with the P-spline model using 20 spline basis functions rather than the P-spline model using five spline basis functions.

Our second research question concerned whether the training dataset and the camera spectral sensitivities affect the performance of colour correction algorithms. For this purpose, two datasets were chosen from different distributions and sizes. We picked Nikon D5100 and Sigma SD Merrill for the cameras. Both spectral sensitivities are publicly available and differ significantly due to colour sensing design \cite{D5100NPL}. For the Nikon, when trained on the larger dataset, we observed that all models but the neural network decreased in performance, even though the testing data set remained the same. Despite the decrease in performance, we also observed a change in the rank order of the models in favour of the P-spline models, which only slightly lost to the neural network. We observed the opposite for Sigma as the performance for P-spline models increased with the larger dataset. At the same time, it decreased in most cases for competitors other than the neural network.

A hypothesis for the varying performance across datasets and cameras is the shape of spectral sensitivities. For the Nikon, the transformation from RGB to XYZ is relatively simple, so a smaller but more evenly distributed dataset might be sufficient since the transformations are also bound to be more straightforward. In the case of Sigma, the transformation is naturally more complex due to the broad shape of spectral sensitivities, and a more extensive dataset might prevent overfitting.

It is also to be noted that while the combination of the Foster and Cave datasets contains more samples than the InSitu dataset, basic nearest neighbour downsampling was used to downscale the large hyperspectral images to a size more suitable for training. Despite this process, we found that there are still excessive amounts of near-duplicate samples, such as the black background in the images in figure \ref{fig:cave}. The authors from \cite{kucuk2023performance} utilised a more elegant method, where they first performed downsampling similarly but followed it by angular threshold filtering for removing similar samples. The InSitu dataset contains hand-picked reflectances from hyperspectral images taken purely for colour correction, and thus, the samples form an even distribution of real-life spectra.

In our third research question, we wanted to gain insight into the underlying transformation performed by our algorithm. This was accomplished by visually examining the partial dependencies and observing the exposure invariance as the penalty $\lambda$ varied. For the Nikon, the transformations were essentially linear, forming a hyperplane in three dimensions with a slight bend. On the contrary, the Sigma camera, as was also noticed in \cite{finlayson2015color}, requires a more aggressive colour correction. Thus, we saw more variation locally in the transformation than in the case of Nikon. As for the exposure invariance, we observed for both cameras that as we increase the penalisation $\lambda$, the models become more robust to changes in exposure. Intuitively, this makes sense as higher values of $\lambda$ drive the fits towards a linear one while lower value allows the fit to vary locally.

To conclude, we presented a novel spline-based model for colour correction that achieves a performance comparable to that of state-of-the-art methods. Our model is flexible in that by tuning a single parameter, $\lambda$, it can be used for cameras with varying spectral sensitivities. Regarding computational complexity, the largest model, with 20 spline basis functions, is similar to neural networks in the number of parameters (coefficients). Apart from the tensor products, our design matrix computation can be implemented efficiently through successive convolution of 0th-order B-splines. In addition, the models are very interpretable since they are linear in terms of their coefficients.

The P-spline models could be improved in numerous ways. Since the idea of P-splines is to employ a large number of basis functions at fixed knot intervals, for computational reasons, it would be wise to explore the usage of fewer basis functions instead but at custom locations and intervals. This is especially true for embedded implementations, where resources may be sparse. Further work could also explore optimising the P-spline models using a perceptual objective function, such as CIEDE2000, using a non-linear algorithm, such as BFGS. In this work, we used basic least-squares optimisation in the XYZ colour space, and as such, the results might not be optimal in the perceptual sense. Finally, more diverse and extensive datasets should be collected with more complex models, such as neural networks, which are becoming mainstream.